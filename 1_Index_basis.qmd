---
title: "1. Prognosemodel - `r params$uitval` `r ifelse(params$propedeusediploma == 'Nvt', '', paste0(' - ', params$propedeusediploma))`"
subtitle: "`r params$faculteit` | `r params$opleidingsnaam` (`r params$opleiding`) - `r params$opleidingsvorm` - versie `r params$versie`"

## Auteur en datum
author: "`r params$author`, De HHs"
date: last-modified

## LTA Template
ltatemplate: 0.9.1.9000

## Format en output
output-file: "lta-hhs-tidymodels-uitval-basis.html"

## Parameters        
params:
  versie: "1.0"
  uitval: "Uitval na 1 jaar"
  propedeusediploma: "Zonder P" ## Nvt/Met P/Zonder P
  faculteit: "GVS"
  
  # GVS:HBO-V
  # opleidingsnaam: "B Opleiding tot Verpleegkundige"
  # opleiding: "HBO-V"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## GVS:MT
  # opleidingsnaam: "B Mens en Techniek"
  # opleiding: "MT"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## GVS:HDT
  # opleidingsnaam: "B Huidtherapie"
  # opleiding: "HDT"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: true
  
  ## FDR:ORM DT
  # opleidingsnaam: "B Ondernemerschap Retail Management"
  # opleiding: "ORM"
  # opleidingsvorm: "deeltijd"
  # opleidingsvorm_afkorting: "DT"
  # selectie: false
  
  ## SWE:SW
  opleidingsnaam: "B Social Work"
  opleiding: "SW"
  opleidingsvorm: "voltijd"
  opleidingsvorm_afkorting: "VT"
  selectie: true
  
  ## TIS:IPO
  # opleidingsnaam: "B Industrieel Product Ontwerpen"
  # opleiding: "IPO"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## TIS:EC
  # opleidingsnaam: "B Elektrotechniek"
  # opleiding: "E"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## TIS:B
  # opleidingsnaam: "B Bouwkunde"
  # opleiding: "B"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## BFM:AC
  # opleidingsnaam: "B International Business"
  # opleiding: "IB-ES-3"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## BFM:AC
  # opleidingsnaam: "B Accountancy"
  # opleiding: "AC"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## ITD:CMD
  # opleidingsnaam: "B Communication and Multimedia Design"
  # opleiding: "CMD"
  # opleidingsvorm: "voltijd"
  # opleidingsvorm_afkorting: "VT"
  # selectie: false
  
  ## Author
  author: "Theo Bakker, lector Learning Technology & Analytics"
  
## Content
includes:
  inleiding:      true
  data:           true
  model_lr:       true
  model_rf:       true
  model_svm:      false
  final_fit:      true
  conclusies:     true
  verantwoording: true
  nextsteps:      true
  copyright:      true
---

```{r setup, include = FALSE}
#| label: setup

## Sluit het _Setup.R bestand in
source("_Setup.R")

## Bepaal op onderdelen of deze getoond moeten worden
bInclude_Model_LR    <- rmarkdown::metadata$includes$model_lr    ## Penalized Logistic Regression
bInclude_Model_RF    <- rmarkdown::metadata$includes$model_rf    ## Random Forest
bInclude_Model_SVM   <- rmarkdown::metadata$includes$model_svm   ## Support Vector Machine
bInclude_Final_fit   <- rmarkdown::metadata$includes$final_fit   ## Final Fit
bInclude_Conclusies  <- rmarkdown::metadata$includes$conclusies  ## Conclusies

## Maak een df om de resultaten per model in op te slaan
dfModel_results <- data.frame(
  model = character(),
  auc = numeric()
)
```

<!-- Inleiding -->

::: {.content-hidden unless-meta="includes.inleiding"}
# Inleiding

## Het nut van prognosemodellen

Prognosemodellen kunnen inzicht bieden in de factoren die gecorreleerd zijn aan de uitval van studenten. Met deze inzichten kan een opleiding interventies ontwikkelen om de uitval te verminderen of te voorkomen. Voor het lectoraat is het van belang dat we in vervolg hierop kunnen analyseren of er sprake is van bias in de data in relatie tot uitval en er mogelijk sprake is van een gebrek aan kansengelijkheid. Een prognosemodel is dus de opmaat naar een fairness analyse. Zie voor een verdere toelichting het onderzoeksprogramma '[No Fairness without Awareness](https://www.dehaagsehogeschool.nl/onderzoek/kenniscentra/no-fairness-without-awareness)'.

## Toelichting op de methode

Voor de ontwikkeling van prognosemodellen gebruiken we de aanpak van [Tidymodels](https://www.tidymodels.org/). Tidymodels is een framework voor het bouwen van een prognosemodel. Hiermee verzekeren we ons van een systematische, herhaalbare en schaalbare aanpak voor het bouwen van prognosemodellen.

## Toelichting op de data

De basis voor deze analyse is studiedata van De Haagse Hogeschool (De HHs), verrijkt door het lectoraat Learning Technology & Analytics. De data bevat informatie over de inschrijvingen van studenten in het eerste jaar van de opleiding:

1.  *Demografische kenmerken*: geslacht, leeftijd, reistijd en SES totaalscore.
2.  *Vooropleidingskenmerken*: toelaatgevende vooropleiding, studiekeuzeprofiel, gemiddeld eindcijfer in de vooropleiding en eventuele deelname aan het Navitas programma.
3.  *Aanmeldingskenmerken*: aansluiting (direct na diploma, tussenjaar, switch), dag van aanmelding, aantal parallelle studies aan De HHs en collegejaar.

## Toelichting op de analyse

We toetsen in deze analyse *`r sUitval_model_text`*, voortaan **Uitval** genoemd.

Uitval is gedefinieerd als het niet meer ingeschreven staan in de opleiding in een aansluitend collegejaar. Een wisseling van opleidingsvorm binnen de opleiding, bijv. van voltijd in jaar 1 naar duaal in jaar 2, geldt niet als uitval.
:::

<!-- Data -->

# Voorbereidingen

## Laad de data

We laden een subset in van historische data specifiek voor:

**Opleiding**: `r params$faculteit` \| `r params$opleidingsnaam` (`r params$opleiding`), `r params$opleidingsvorm`, eerstejaars - **`r sUitval_model`**

```{r}
#| label: load_data

## Laad de data voor de opleiding
dfOpleiding_inschrijvingen_base <- get_lta_studyprogram_enrollments_pin(
    board = "HHs/Inschrijvingen",
    faculty = faculteit,
    studyprogram = opleidingsnaam_huidig,
    studytrack = opleiding,
    studyform = toupper(opleidingsvorm),
    range = "eerstejaars")

## Herschik de levels
Set_Levels(dfOpleiding_inschrijvingen_base)

dfOpleiding_inschrijvingen_base <- dfOpleiding_inschrijvingen_base |>  
  
  ## Maak een eenvoudige Uitval variabele aan
  Mutate_Uitval(sUitval_model) |>
  
  ## Maak van de uitval variabele een factor
  mutate(SUC_Uitval = as.factor(SUC_Uitval)) |> 

  ## Verbijzonder eventueel op basis van het propedeusediploma
  Filter_Propedeusediploma(sPropedeusediploma) |>

  ## Maak van de Dubbele studie variabele een Ja/Nee variabele
  mutate(INS_Dubbele_studie = ifelse(INS_Aantal_inschrijvingen > 1, "Ja", "Nee")) |>  

  ## Verwijder INS_Aantal_inschrijvingen
  select(-INS_Aantal_inschrijvingen) |> 

  ## Pas voor een aantal variabelen de levels aan
  Mutate_Levels(
  c(
    "VOP_Studiekeuzeprofiel_LTA_afkorting",
    "INS_Aansluiting_LTA",
    "VOP_Toelaatgevende_vooropleiding_soort"
  ),
    list(lLevels_skp, lLevels_vop, lLevels_vop)
  )
  
## B Huidtherapie: Filter op uitsluitend studenten met een rangnummer (selectie)
if(opleiding == "HDT") {
  dfOpleiding_inschrijvingen_base <- dfOpleiding_inschrijvingen_base |> 
    filter(!is.na(RNK_Rangnummer)) 
} 

```

## Selecteer en inspecteer de data

We selecteren eerst de relevante variabelen. We verwijderen daarbij variabelen die maar 1 waarde hebben. We bekijken de variabelen in een samenvatting in relatie tot de Uitval. Daarnaast bekijken we de kwaliteit van de data op missende waarden.

```{r}
#| label: select_inspect_data

lSelect <- c(
    "INS_Student_UUID_opleiding_vorm",
    "CBS_APCG_tf",
    "DEM_Geslacht",
    "DEM_Leeftijd_1_oktober",
    "GIS_Tijd_fiets_OV",
    "INS_Collegejaar",
    "INS_Dagen_tussen_aanmelding_en_1_september",
    "INS_Dubbele_studie",
    "INS_Aansluiting_LTA",
    "INS_Navitas_tf",
    "SES_Deelscore_arbeid",
    "SES_Deelscore_welvaart",
    "SES_Totaalscore",
    "SUC_Uitval",
    "VOP_Gemiddeld_cijfer_cijferlijst",
    "VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO",
    "VOP_Cijfer_CE1_nederlands",
    "VOP_Cijfer_CE1_engels",
    "VOP_Cijfer_CE_proxy_wiskunde",
    "VOP_Cijfer_CE1_natuurkunde",
    "VOP_Studiekeuzeprofiel_LTA_afkorting",
    "VOP_Toelaatgevende_vooropleiding_soort"
  )

## B Huidtherapie: voeg de variabele RNK_Rangnummer toe
if(opleiding == "HDT") {
  lSelect <- c(lSelect, "RNK_Rangnummer")
}

## Maak een subset
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen_base |>
  
  ## Selecteer de relevante variabelen
  select_at(lSelect) |>
  
  ## Hernoem variabelen voor beter leesbare namen
  rename(
    ID                    = INS_Student_UUID_opleiding_vorm,
    Geslacht              = DEM_Geslacht,
    Leeftijd              = DEM_Leeftijd_1_oktober,
    Reistijd              = GIS_Tijd_fiets_OV,
    Dubbele_studie        = INS_Dubbele_studie,
    Collegejaar           = INS_Collegejaar,
    Aanmelding            = INS_Dagen_tussen_aanmelding_en_1_september,
    Aansluiting           = INS_Aansluiting_LTA,
    Navitas               = INS_Navitas_tf,
    APCG                  = CBS_APCG_tf,
    SES_Arbeid            = SES_Deelscore_arbeid,
    SES_Welvaart          = SES_Deelscore_welvaart,
    SES_Totaal            = SES_Totaalscore,          
    Uitval                = SUC_Uitval,
    Cijfer_SE_VO          = VOP_Gemiddeld_cijfer_cijferlijst,
    Cijfer_CE_VO          = VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO,
    Cijfer_CE_Nederlands  = VOP_Cijfer_CE1_nederlands,
    Cijfer_CE_Engels      = VOP_Cijfer_CE1_engels,
    Cijfer_CE_Wiskunde    = VOP_Cijfer_CE_proxy_wiskunde,
    Cijfer_CE_Natuurkunde = VOP_Cijfer_CE1_natuurkunde,
    Studiekeuzeprofiel    = VOP_Studiekeuzeprofiel_LTA_afkorting,
    Vooropleiding         = VOP_Toelaatgevende_vooropleiding_soort
  ) |> 
  
  ## Pas CBS_APCG_tf aan naar factor
  mutate(APCG = case_when(APCG == TRUE ~ "Ja",
                          APCG == FALSE ~ "Nee",
                          .default = "Onbekend")) |>

  ## Geef aan waar missende cijfers in het VO zijn
  Mutate_Cijfers_VO() |>
  
  ## Verwijder variabelen, waarbij er maar 1 waarde is
  select(where(~ n_distinct(.) > 1)) |>
  
  arrange(Collegejaar, ID)

## B Huidtherapie: hernoem de variabele RNK_Rangnummer
if(opleiding == "HDT") {
  dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
    rename(Rangnummer = RNK_Rangnummer)
} 

dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
 ltabase::sort_distinct()

## Verwijder de basis dataset
rm(dfOpleiding_inschrijvingen_base)

```

```{r, echo=FALSE, results='asis'}
#| label: summary_data

## Maak een samenvatting van de data
dfSummary <- dfOpleiding_inschrijvingen |>
  
  ## Verwijder kolommen die niet relevant zijn voor de analyse
  select(-c(ID, Collegejaar)) |> 
  
  ## Pas de labels van Uitval aan van True naar Ja, en van False naar Nee
  mutate(Uitval = fct_recode(Uitval, "Nee" = "FALSE", "Ja" = "TRUE")) |>
  
  ## Pas de volgorde van de labels van Uitval aan
  mutate(Uitval = fct_relevel(Uitval, "Ja", "Nee"))

## Toon deze als een samenvattende tabel
tbl_dfSummary <- dfSummary |> 
  
  tbl_summary(
    by = Uitval,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = all_continuous() ~ 2, 
    missing = "no",
    percent = "row"
  ) |> 
  
  ## Richt de vormgeving van de table in
  modify_header(all_stat_cols() ~ "**{level}**, N={n} ({style_percent(p)}%)") |>
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Uitval**") |>
  modify_header(label = "**Variabele**") |>
  bold_labels() |>
  modify_caption("**Studentkenmerken versus Uitval**") |>
  suppressWarnings(add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2),
        test.args = all_tests("fisher.test") ~ list(simulate.p.value = TRUE))) |>
  add_overall(last = TRUE, col_label = "**Totaal**, N = {N}")

tbl_dfSummary
  
```

```{r}
#| label: summarize_data

## Laad dlookr
suppressMessages(library(dlookr))

## Toon een samenvatting van de data, gesorteerd op missende waarden
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(missing_percent, 2)) |>
  arrange(desc(missing_percent)) |>
  knitr::kable(caption = "Kwaliteit van de data voor bewerkingen (gesorteerd op missende waarden)")

## Verwijder dlookr
detach("package:dlookr", unload = TRUE)

```

## Bewerk de data

-   Uit de eerste diagnose blijkt dat niet alle variabelen goed genoeg zijn voor het bouwen van een prognosemodel: er zijn missende waarden en niet alle veldtypes zijn geschikt. We passen de variabelen aan zodat we in het model er goed mee kunnen werken.
-   Prognosemodellen kunnen niet omgaan met missende waarden. Om bias te voorkomen verwijderen we geen rijen met missende waarden, maar vullen die op (*imputatie*). We bewerken de data zo dat alle missende waarden worden opgevuld: bij numerieke waarden met het gemiddelde en bij categorische variabelen met 'Onbekend'.
-   We passen sommige variabelen aan, zodat ze in het model gebruikt kunnen worden: tekstvelden zetten we om naar factor; logische variabelen (Ja/Nee) zetten we om naar een numerieke variabele (1/0).
-   De uitkomstvariabele, `Uitval`, leiden we af van de variabele `SUC_Uitval_aantal_jaar_LTA`. Als de waarde daar 1 is, is de student na 1 jaar uitgevallen, 2 na 2 jaar, etc.
-   Een fictief studentnummer (`INS_Student_UUID_opleiding_vorm`) gebruiken we ook, zodat we - als er afwijkende resultaten zijn - de dataset gericht kunnen onderzoeken indien nodig.

```{r}
#| label: mutate_data

## Bewerk de data
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
  
  ## Imputeer alle numerieke variabelen met de mean
  mutate(across(where(is.numeric), ~ ifelse(
    is.na(.x),
    mean(.x, na.rm = T),
    .x
  )) ) |>
  
  ## Zet character variabelen om naar factor
  mutate(across(where(is.character), as.factor)) |> 
  
  ## Zet logische variabelen om naar 0 of 1
  mutate(across(where(is.logical), as.integer)) |>
  
  ## Vul in factoren missende waarden op met "Onbekend"
  mutate(across(where(is.factor), ~ suppressWarnings(
    fct_explicit_na(.x, na_level = "Onbekend")
  ))) |> 
  
  ## Herschik de kolommen, zodat Uitval vooraan staat
  select(Uitval, everything()) 

## Bekijk de data
## glimpse(dfOpleiding_inschrijvingen) 

## Laad dlookr
suppressMessages(library(dlookr))

## Maak een diagnose van de data
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(unique_rate, 2)) |>
  knitr::kable(caption = "Kwaliteit van de data na bewerkingen")

detach("package:dlookr", unload = TRUE)

```

## Bekijk de onderlinge correlaties

Het is verstandig om voorafgaand aan het bouwen van een model te kijken naar de onderlinge correlaties tussen numerieke variabelen. Dit geeft inzicht in de data en kan helpen bij het maken van keuzes voor het model of de duiding van de uitkomsten.

```{r}
#| label: corplot_data

## Maak een plot van de onderlinge correlaties in numerieke variabelen
dfOpleiding_inschrijvingen |> 
  select(-Collegejaar) |>
  select(where(is.numeric)) |> 
  cor() |> 
  corrplot::corrplot(
    order = 'hclust', 
    addrect = 4,
    method = "number",  
    tl.cex = 0.8,       
    tl.col = "black",
    diag = FALSE)
```

## Bouw de trainingset, testset en validatieset

-   De data is nu geschikt om een prognosemodel mee te bouwen.
-   Om het model te bouwen, testen en valideren, splitsen we de data in drie delen van 60%, 20% en 20%. We doen dit op zo'n manier, dat elk deel ongeveer een gelijk aantal studenten bevat dat uitvalt.
-   We trainen het model op basis van 60% en testen de modellen tijden het trainen op de overige 20% (de testset).
-   Als het model klaar is, valideren we het op de 20% studenten uit de validatieset, die is opgebouwd uit zo recent mogelijke data. De validatieset blijft dus de gehele tijd ongemoeid, zodat we overfitting - een te goed model op bekende data, maar slechte performance op onbekende data - voorkomen.
-   Een willekeurig, maar vaststaand seedgetal voorkomt dat we bij elke run van het model c.q. deze code een net iets andere uitkomst krijgen.

```{r}
#| label: split_data

set.seed(0821)

## Splits de data in 3 delen: 60%, 20% en 20%
splits      <- initial_validation_split(dfOpleiding_inschrijvingen,
                                        strata = Uitval,
                                        prop = c(0.6, 0.2))

## Maak drie sets: een trainingset, een testset en een validatieset
dfUitval_train      <- training(splits)
dfUitval_test       <- testing(splits)
dfUitval_validation <- validation_set(splits)

## Maak een resample set op basis van 10 folds (default)
dfUitval_resamples  <- vfold_cv(dfUitval_train, strata = Uitval)

## Training set proporties uitval
dfUitval_train |> 
  count(Uitval) |> 
  mutate(prop = n/sum(n)) |> 
  knitr::kable(caption = "Trainingsset") 

## Test set proporties uitval
dfUitval_test  |> 
  count(Uitval) |> 
  mutate(prop = n/sum(n)) |> 
  knitr::kable(caption = "Testset") 

```

<!-- MODEL I: Penalized Logistic Regression -->

::: {.content-hidden unless-meta="includes.model_lr"}
# Model I: Logistische Regressie

-   Het eerste model is een [logistische regressie met penalized likelihood](https://wikistatistiek.amc.nl/Logistische_regressie); we gebruiken de `glmnet` engine voor het bouwen van het model. Penalized likelihood is een techniek die helpt bij het voorkomen van overfitting. [Glmnet](https://glmnet.stanford.edu/articles/glmnet.html) is een populair package voor het bouwen van logistische regressiemodellen.
-   We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric.

## Maak het model

We bouwen eerst het model.

```{r}
#| label: lr_mod
#| code-fold: false

## Bouw het model: logistische regressie
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
```

## Maak de recipe

Vervolgens zetten we meerdere stappen in een 'recipe':

-   We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model.
-   We converteren factoren naar dummy variabelen.
-   We verwijderen variabelen die geen waarde toevoegen: variabelen met enkel nullen.
-   We transformeren numerieke variabelen om ze met elkaar te kunnen vergelijken door ze te centreren en schalen.
-   We controleren tot slot of er geen missende waarden zijn.
-   Sterk gecorreleerde waarden verwijderen we nu niet, omdat we later in de analyse de eventuele samenhang met andere variabelen in een prognosemodel nog willen kunnen visualiseren.

```{r}
#| label: lr_recipe
#| code-fold: false

## Bouw de recipe: logistische regressie
lr_recipe <- 
  recipe(Uitval ~ ., data = dfUitval_train) |>  
  update_role(ID, new_role = "ID") |>           ## Zet de student ID als ID variabele
  step_rm(ID, Collegejaar) |>                   ## Verwijder ID en collegejaar uit het model
  step_dummy(all_nominal_predictors()) |>       ## Maak dummy variabelen van categorische variabelen
  step_zv(all_predictors()) |>                  ## Verwijder zero values
  step_normalize(all_numeric_predictors()) |>   ## Centreer en schaal numerieke variabelen
  check_missing(everything())                   ## Controleer dat er geen missende waarden zijn

## Toon de recipe
tidy(lr_recipe) |> 
  knitr::kable()

## Toon de variabelen die nog resteren
model_vars <- lr_recipe |> 
  prep() |> 
  juice() |> 
  names()

## Voeg lege waarden toe om de lengte deelbaar door 3 te maken
while (length(model_vars) %% 3 != 0) {
  model_vars <- c(model_vars, "")
}

model_vars_matrix <- matrix(model_vars, ncol = 3, byrow = FALSE)

knitr::kable(model_vars_matrix, caption = "Resterende variabelen na preprocessing")


```

## Maak de workflow

Voor de uitvoering bouwen we een nieuwe workflow. Daaraan voegen we het model en de bewerkingen in de recipe toe.

```{r}
#| label: lr_workflow
#| code-fold: false

## Maak de workflow: logistische regressie
lr_workflow <- 
  workflow() |>         ## Maak een workflow
  add_model(lr_mod) |>  ## Voeg het model toe
  add_recipe(lr_recipe) ## Voeg de recipe toe

## Toon de workflow
lr_workflow
```

## Tune en train het model

Het model moet getuned worden. Dit houdt in dat we de beste parameters voor het model moeten vinden. We maken een grid met verschillende penalty waarden. Daarmee kunnen we vervolgens het beste model selecteren met de hoogste ROC/AUC. We plotten de resultaten van de tuning, zodat we hieruit het beste model kunnen kiezen.

```{r}
#| label: lr_reg_grid
#| code-fold: false

## Maak een grid: logistische regressie
lr_reg_grid <- tibble(penalty = 10 ^ seq(-4, -1, length.out = 30))

## Train en tune het model: logistische regressie
lr_res <- 
  lr_workflow |> 
  tune_grid(dfUitval_validation,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}
#| label: lr_plot

## Plot de resultaten + een rode verticale lijn voor de max AUC
lr_plot <- 
  lr_res |> 
  collect_metrics() |> 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number()) +
    theme(
      axis.title.x = element_text(margin = margin(t = 20))
    )

# Zoek de penalty waarde met de max AUC
max_auc_penalty <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)

# Voeg de rode verticale lijn toe aan lr_plot
lr_plot_plus <- lr_plot + 
  geom_vline(xintercept = max_auc_penalty, color = "red")

# Vindt een mean voor de max AUC die hoger is
max_auc_mean <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)


lr_plot_plus

```

## Kies het beste model

We evalueren modellen met een zo hoog mogelijke Area under the ROC Curve (AUC/ROC) en een zo laag mogelijke penalty. Zo kunnen we uit de resultaten het beste model kiezen. Tot slot maken we een ROC curve om de prestaties van het model te visualiseren.

```{r}
#| label: lr_top_models
#| code-fold: false

## Toon het beste model
top_models <-
  lr_res |> 
  show_best(metric = "roc_auc", n = 10) |> 
  mutate(mean = round(mean, 6)) |>
  arrange(penalty) 

top_models|> 
  knitr::kable()

```

```{r}
#| label: lr_best
#| code-fold: false

## Selecteer het beste model: logistische regressie
lr_best <- 
  lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |>
  slice(1) 

lr_best|> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable()

```

```{r}
#| label: lr_auc
#| code-fold: false

## Verzamel de predicties en evalueer het model (AUC/ROC): logistische regressie
lr_auc <- 
  lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  mutate(model = "Logistisch Regressie")

## Plot de ROC curve
Get_ROC_plot(lr_auc, position = 1)

## Bepaal de AUC van het beste model
lr_auc_highest   <-
  lr_res |>
  collect_predictions(parameters = lr_best) |> 
  roc_auc(Uitval, .pred_FALSE)

## Voeg de naam van het model en de AUC toe dfModel_results
dfModel_results <- 
  dfModel_results |>
  add_row(model = "Logistic Regression", auc = lr_auc_highest$.estimate)

```
:::

<!-- MODEL II: Random Forest -->

::: {.content-hidden unless-meta="includes.model_rf"}
# Model II: Tree-based ensemble

-   Het tweede model is een [random forest](https://en.wikipedia.org/wiki/Random_forest): een ensemble van beslisbomen (decision trees). Het is een krachtig model dat goed om kan gaan met complexe data en veel variabelen.
-   We gebruiken de `ranger` engine voor het bouwen van het model.

## Bepaal het aantal PC-cores

Omdat een random forest model veel berekeningen vereist, willen we daarvoor alle computerkracht gebruiken die beschikbaar is. Het aantal CPU's (*cores*) van de computer bepaalt hoe snel het model getraind kan worden. Deze informatie gebruiken we bij het bouwen van het model.

```{r}
#| label: cores

## Bepaal het aantal cores
cores <- parallel::detectCores()

```

## Maak het model

We bouwen eerst het model. We gebruiken de `rand_forest` functie om het model te bouwen. We tunen de `mtry` en `min_n` parameters. De `mtry` parameter bepaalt het aantal variabelen dat per boom wordt gebruikt. De `min_n` parameter bepaalt het minimum aantal observaties dat in een blad van de boom moet zitten. De functie `tune()` is hier nog een *placeholder* om de beste waarden voor deze parameters - die we later bepalen - daar in te stellen. We gebruiken 1.000 bomen c.q. versies van het model.

```{r}
#| label: rf_mod
#| code-fold: false

## Bouw het model: random forest

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = cores) |> 
  set_mode("classification")
```

## Maak de recipe

We maken een recipe voor het random forest model. We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model. Overige stappen zijn bij een random forest minder relevant in tegenstelling tot een regressiemodel.

```{r}
#| label: rf_recipe
#| code-fold: false

## Maak de recipe: random forest
rf_recipe <- 
  recipe(Uitval ~ ., data = dfUitval_train) |> 
  step_rm(ID, Collegejaar)                      ## Verwijder ID en Collegejaar uit het model
  
## Toon de recipe
tidy(rf_recipe) |> 
  knitr::kable()
```

## Maak de workflow

We voegen het model en de recipe toe aan de workflow voor dit model.

```{r}
#| label: rf_workflow
#| code-fold: false

## Maak de workflow: random forest
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe)

## Toon de workflow
rf_workflow
```

## Tune en train het model

We trainen en tunen het model in de workflow. We maken een grid met verschillende waarden voor de parameters `mtry` en `min_n`. We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric. Met de resultaten van de tuning kiezen we het beste model.

```{r}
#| label: rf_tune
#| code-fold: false

## Toon de parameters die getuned kunnen worden
rf_mod

## Extraheer de parameters die getuned worden
extract_parameter_set_dials(rf_mod)

## Bepaal de seed
set.seed(2904)

## Bouw het grid: random forest
rf_res <- 
  rf_workflow |> 
  tune_grid(dfUitval_validation,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

## Kies het beste model

We evalueren de beste modellen en maken een ROC curve om de performance van het model te visualiseren. Vervolgens vergelijken we de prestaties van de modellen en kiezen we het beste model.

```{r}
#| label: rf_results
#| code-fold: false

## Toon de beste modellen
rf_res |> 
  show_best(metric = "roc_auc", n = 15) |> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable()

## Plot de resultaten
autoplot(rf_res) 

```

```{r}
#| label: rf_best
#| code-fold: false

## Selecteer het beste model
rf_best <- 
  rf_res |> 
  select_best(metric = "roc_auc")

rf_best|> 
  knitr::kable()

```

```{r}
#| label: rf_auc
#| code-fold: true

## Verzamel de predicties
rf_res |> 
  collect_predictions() |> 
  head(10) |>
  knitr::kable()

## Bepaal de AUC/ROC curve
rf_auc <- 
  rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  mutate(model = "Random Forest")

## Plot de ROC curve
Get_ROC_plot(rf_auc, position = 2)

## Bepaal de AUC van het beste model
rf_auc_highest   <-
  rf_res |>
  collect_predictions(parameters = rf_best) |> 
  roc_auc(Uitval, .pred_FALSE)

## Voeg de naam van het model en de AUC toe dfModel_results
dfModel_results <- 
  dfModel_results |>
  add_row(model = "Random Forest", auc = rf_auc_highest$.estimate)

```
:::

<!-- Final Fit -->

# De uiteindelijke fit

-   In de laatste stap van deze analyse maken we het model definitief.
-   We testen het model op de testset en evalueren het model met metrieken en de Variable Importance Factor (VIF).

## Combineer de AUC/ROC curves en kies het beste model

Eerst combineren we de AUC/ROC curves van de modellen om ze te vergelijken. We kiezen het beste model op basis van de hoogste AUC/ROC.

```{r}
#| label: bind_rows_auc_roc

## Combineer de AUC/ROC curves om de modellen te vergelijken
Get_ROC_plot(list(lr_auc, rf_auc))


```

```{r}
#| label: best_model_auc_roc

## Bepaal welke van de modellen het beste is op basis van de hoogste AUC/ROC
dfModel_results <- dfModel_results |>
  mutate(number = row_number()) |> 
  mutate(best = ifelse(auc == max(auc), TRUE, FALSE)) |> 
  arrange(number)

## Bepaal het beste model
sBest_model <- dfModel_results$model[dfModel_results$best == TRUE]
sBest_model_auc <- round(dfModel_results$auc[dfModel_results$best == TRUE], 4)
```

```{r, echo=FALSE, results='asis'}
#| label: best_model_auc_roc_text

## Bouw de tekst voor de beste modellen op
sText_best_model <- ""

for (i in 1:nrow(dfModel_results)) {
  sText_best_model <-
    glue(
      sText_best_model,
      "Het {dfModel_results[i,]$model} model heeft een AUC van {round(dfModel_results[i,]$auc, 4)}. "
    )
}

## Herschik de uiteindelijke tekst voor een heldere samenvatting
sText_best_model <-
  glue(
    "Het beste model is het **{sBest_model}** model met een **AUC/ROC van {sBest_model_auc}**. {sText_best_model} We ronden de analyse verder af met het {sBest_model} model."
  )

```

`r sText_best_model`

## Maak het finale model

```{r, echo=FALSE, results='asis'}
#| label: text_final_model

## Bouw de tekst voor het laatste model op
if(sBest_model == "Logistisch Regressie") {
  
  ## Maak het laatste model
  sText_final_model <- "We maken het finale model op basis van de beste parameters die we hebben gevonden. Voor het Logistisch Regressie model is dit het model met de beste penalty en mixture."

} else if (sBest_model == "Random Forest") {
  
  ## Maak het laatste model
  sText_final_model <- "We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren."

}
```

We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren.

```{r}
#| label: last_mod
#| code-fold: false

## Test het ontwikkelde model op de testset
## Bepaal de optimale parameters

## Bouw de laatste modellen
last_lr_mod <-
    logistic_reg(penalty = lr_best$penalty,
                 mixture = 1) |>
    set_engine("glmnet") |>
    set_mode("classification")

last_rf_mod <-
    rand_forest(mtry = rf_best$mtry,
                min_n = rf_best$min_n,
                trees = 1000) |>
    set_engine("ranger", num.threads = cores, importance = "impurity") |>
    set_mode("classification")

```

## Maak de workflow

We voegen het model toe aan de workflow en updaten de workflow met het finale model.

```{r}
#| label: last_workflow
#| code-fold: false

## Update de workflows
 last_lr_workflow <- 
    lr_workflow |> 
    update_model(last_lr_mod)

 last_rf_workflow <- 
    rf_workflow |> 
    update_model(last_rf_mod)

```

## Fit het finale model

We voeren de finale fit uit. De functie `last_fit` past het model toe op de validatieset.

```{r}
#| label: last_fit
#| code-fold: false

## Voer de laatste fit uit
set.seed(2904)

## Maak voor beide modellen een laatste fit, zodat we deze kunnen opslaan voor later gebruik
last_fit_lr <- 
    last_lr_workflow |> 
    last_fit(splits)

last_fit_rf <- 
    last_rf_workflow |> 
    last_fit(splits)

lLast_fits <- list(last_fit_lr, last_fit_rf) |> 
  set_names(c("Logistic Regression", "Random Forest"))

## Bepaal welk model het beste is
if(sBest_model == "Logistic Regression") {
  last_fit <- last_fit_lr
} else if(sBest_model == "Random Forest") {
  last_fit <- last_fit_rf
}

## Bewaar de resultaten, de modelresultaten en de bijbehorende data
sFittedmodels_outputpath <- Get_Model_outputpath(mode = "last-fits")
saveRDS(lLast_fits, file = sFittedmodels_outputpath)

sModelresults_outputpath <- Get_Model_outputpath(mode = "modelresults")
saveRDS(dfModel_results, file = sModelresults_outputpath)

sData_outputpath <- Get_Model_outputpath(mode = "data")
saveRDS(dfOpleiding_inschrijvingen, file = sData_outputpath)

```

## Evalueer het finale model: metrieken en vif

We evalueren het finale model op basis van 3 metrieken (accuraatheid, ROC/AUC en de [Brier score](https://en.wikipedia.org/wiki/Brier_score) = de Mean Squared Error) en de Variable Importance Factor (VIF). Uit de VIF is op te maken welke variabelen het meest bijdragen aan de voorspelling van de uitkomstvariabele.

```{r}
#| label: last_fit_metrics_vif
#| code-fold: false

## Verzamel de metrieken
last_fit |> 
  collect_metrics() |> 
  mutate(.estimate = round(.estimate, 4)) |>
  knitr::kable()

## Extraheer de feature importance
last_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 20) +
  ggtitle("Meest voorspellende factoren") +
  labs(y = "Belangrijkheid") +
  theme(
    axis.title.x = element_text(margin = margin(t = 20))  
  )

```

## Plot de ROC curve

Tot slot maken we een ROC curve om de prestaties van het definitieve model te visualiseren. De Sensitivity (True Positive Rate) en Specificity (True Negative Rate) worden hierin uitgezet. De Area under the ROC Curve (AUC/ROC) geeft de prestaties van het model weer. Het model scoort beter naarmate de AUC/ROC dichter bij de 1 ligt (in de linkerbovenhoek). Een AUC/ROC van 0,5 betekent dat het model niet beter presteert dan een willekeurige voorspelling.

```{r}
#| label: last_fit_roc
#| code-fold: false

## Toon de roc curve
auc_lf <- last_fit |> 
  collect_predictions() |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  mutate(model = "Last fit")

Get_ROC_plot(auc_lf, position = 3)

```

<!-- Conclusies -->

::: {.content-hidden unless-meta="includes.conclusies"}
# Conclusies

```{r, echo = FALSE}
#| label: conclusions_accuracy
#| code-fold: false

## Bepaal de accuraatheid van het model, het gemiddelde uitvalpercentage en het base-model
Last_fit_Accuracy   <- last_fit |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pull(.estimate) |>
  round(4) * 100
Avg_Uitval          <- round(mean(dfOpleiding_inschrijvingen$Uitval == TRUE) * 100, 2) 
Base_Model_Accuracy <- round(100 - Avg_Uitval, 2)

if(Avg_Uitval < 50) {
  Base_Model_Accuracy <- round(100 - Avg_Uitval, 2)
  sUitval <- "die niet uitviel"
} else {
  Base_Model_Accuracy <- Avg_Uitval
  sUitval <- "die uitviel"
}

## Bereken nu het verschil in accuraatheid
nAccuracy_verschil  <- round(abs(Last_fit_Accuracy - Base_Model_Accuracy), 2)
pAccuracy_verschil  <- paste0(nAccuracy_verschil, "%") 

## Functies
Get_Accuracy_vergelijking <-
  function(Last_fit_Accuracy, Base_Model_Accuracy) {
    if (Last_fit_Accuracy == Base_Model_Accuracy) {
      "even goed als"
    } else if (Last_fit_Accuracy > Base_Model_Accuracy) {
      "beter"
    } else {
      "slechter"
    }
  }
  
Get_Accuracy_niveau <- function(Last_fit_Accuracy) {
  if (Last_fit_Accuracy > 95) {
    "zeer hoog"
  } else if (Last_fit_Accuracy > 90) {
    "hoog"
  } else if (Last_fit_Accuracy > 80) {
    "vrij hoog"
  } else if (Last_fit_Accuracy > 70) {
    "gemiddeld"
  } else if (Last_fit_Accuracy > 60) {
    "vrij laag"
  } else {
    "laag"
  }
}
  
Get_Accuracy_verschil <- function(nAccuracy_verschil) {
  if (nAccuracy_verschil < 5) {
    "iets"
  } else if (nAccuracy_verschil < 10) {
    "wat"
  } else if (nAccuracy_verschil < 20) {
    "een stuk"
  } else {
    "veel"
  }
}

## Functies om zinnen te maken en variabelen te printen
Special_Paste  <- function(vec) sub(",\\s+([^,]+)$", " en \\1", toString(vec))
Print_Variable <- function(variable) paste0("`", variable, "`")

## Bepaal een aantal teksten
sAccuracy_vergelijking <- Get_Accuracy_vergelijking(Last_fit_Accuracy, Base_Model_Accuracy)
sAccuracy_niveau       <- Get_Accuracy_niveau(Last_fit_Accuracy)
sAccuracy_verschil     <- Get_Accuracy_verschil(nAccuracy_verschil)

## Bepaal de top 3 en top 5 variabelen op basis van de VIF
dfTop_vif   <- last_fit |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  arrange(desc(Importance)) 

lTop3_vif <- dfTop_vif |>
  slice(1:3) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()  

lTop45_vif <- dfTop_vif |>
  ## Regel 4 en 5
  slice(4:5) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()

sConclusie_text <- ""

if(!("Eindcijfer_VO" %in% dfTop_vif$Variable[1:3]) 
   & "Eindcijfer_VO" %in% dfTop_vif$Variable[4:5]) {
  sConclusie_text <- glue::glue(
    sConclusie_text,
    "**Het model toont wel aan dat de herkomst van studenten sterker is gecorreleerd met {tolower(sUitval_model)} dan eerdere prestaties of vooropleiding.** ",
    "De _Variable Importance Factor_ (VIF) laat namelijk zien dat de variabelen {lTop3_vif} de 3 belangrijkste variabelen zijn voor het voorspellen van {tolower(sUitval_model)}, gevolgd door {lTop45_vif}."
  )
} else if ("Eindcijfer_VO" %in% dfTop_vif$Variable[1:3]) {
  sConclusie_text <- glue::glue(
    sConclusie_text,
    "**Het model toont aan dat eerdere prestaties van studenten sterker zijn gecorreleerd met {tolower(sUitval_model)} dan herkomst.** ",
    "De _Variable Importance Factor_ (VIF) laat namelijk zien dat de variabelen {lTop3_vif} de 3 belangrijkste variabelen zijn voor het voorspellen van {tolower(sUitval_model)}, gevolgd door {lTop45_vif}."
  )
} else {
  sConclusie_text <- "**Vooralsnog geen nadere bijzonderheden.**"
}

```

## Het beste prognosemodel voor deze opleiding

**Het beste prognosemodel blijkt het `r sBest_model` model te zijn.**

-   Van de prognosemodellen die we hebben ontwikkeld om `r tolower(sUitval_model)` te voorspellen, had het `r sBest_model` model de hoogste AUC/ROC waarde (`r sBest_model_auc`).

## Mate van accuraatheid en lift

Een prognosemodel moet minimaal beter presteren dan een *base-model* om waarde op accuraatheid toe te voegen. Het base-model neemt de grootste klasse van de gemiddelde `r tolower(sUitval_model)` van de afgelopen jaren als basis. Stel we zouden tegen alle studenten zeggen dat ze hun studie gaan halen, dan is de mate van accuratesse gelijk aan dit base-model. Dit base-model is dus altijd hoger dan de 50% lijn van de AUC/ROC curve, tenzij het base-model toevallig precies 50% is.

**De mate van accuraatheid van de toepassing van het model is `r sAccuracy_niveau` (`r Last_fit_Accuracy`%).**

-   **Base-model: `r Base_Model_Accuracy`%** -- Voor deze opleiding bereken we het base-model als volgt. Van alle studenten viel `r Avg_Uitval`% uit. De grootste klasse (en de accuratesse) van het base-model is daarmee (100% - `r Avg_Uitval`% = ) `r Base_Model_Accuracy`% `r sUitval`.
-   **Accuratesse prognose: `r Last_fit_Accuracy`%** -- Het model voorspelt `r sUitval_model` met een accuratesse van `r Last_fit_Accuracy`%.
-   **Lift: `r pAccuracy_verschil`** -- Het model scoort in de huidige opbouw met een verschil van `r pAccuracy_verschil` (de *lift*) `r sAccuracy_verschil` `r sAccuracy_vergelijking` dan de accuraatheid van het base-model (`r Base_Model_Accuracy`%).

## Confusion Matrix

```{r}
#| label: confusion_matrix_calculation

## Bepaal de confusion matrix
confusion_matrix <- last_fit |>
  collect_predictions() |>
  conf_mat(truth = Uitval, estimate = .pred_class) 

dfConf_matrix <- as_tibble(confusion_matrix$table) |>
  rename(Werkelijkheid = Truth) |>
  mutate(Werkelijkheid = ifelse(Werkelijkheid == "TRUE", "Uitval", "Geen uitval"),
         Prediction = ifelse(Prediction == "TRUE", "Uitval", "Geen uitval"))

pTP <- Number_to_readable((dfConf_matrix$n[1]/sum(dfConf_matrix$n)*100),1)
pFP <- Number_to_readable((dfConf_matrix$n[3]/sum(dfConf_matrix$n)*100),1)
pTN <- Number_to_readable((dfConf_matrix$n[2]/sum(dfConf_matrix$n)*100),1)
pFN <- Number_to_readable((dfConf_matrix$n[4]/sum(dfConf_matrix$n)*100),1)
pACC <- Number_to_readable(Last_fit_Accuracy,1)


```

De prestaties van het model kunnen we verder uitdrukken in een *confusion matrix*. Hierin zien we de voorspellingen van het model en de werkelijke uitkomsten. De matrix geeft inzicht in de mate van correcte en incorrecte voorspellingen. De accuraatheid van het model berekenen we door het aandeel goed voorspelde uitkosten, Uitval = Uitval (*True Positive*) en Geen uitval = Geen uitval (*True Negative*), af te zetten tegen het totaal aantal voorspellingen: `r pTP`% + `r pTN`% = `r pACC`%.

```{r}
#| label: confusion_matrix_plot

plot_confusion_matrix(dfConf_matrix, 
                      target_col = "Werkelijkheid", 
                      prediction_col = "Prediction",
                      counts_col = "n",
                      palette = "Blues",
                      add_sums = TRUE,
                      theme_fn = ggplot2::theme_light,
                      sums_settings = sum_tile_settings(
                        palette = "Greens",
                        label = "Totaal",
                        tc_tile_border_color = "black"
                      )) +
  ## Pas de labels aan
  labs(title = "Confusion Matrix",
       x = "Werkelijkheid",
       y = "Voorspelling") +
  
  Set_LTA_Theme()

```

## Uitleggen of verklaren?

Naast de accuraatheid van het model is het ook belangrijk om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(sUitval_model)`. Daarin gaat de vergelijking met de prestaties van het basemodel mank. Dat model geeft op geen enkele manier aan waarom een student een kans op succes of uitval heeft, anders dan - 'dit is gebruikelijk in deze opleiding'.

Ongeacht de mate van accuraatheid, is het voor ons onderzoek naar kansengelijkheid essentieel om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(sUitval_model)`. Het gaat erom dat we het belang van de factoren in de voorspellingen kunnen begrijpen en duiden. Machine Learning is hiervoor uitstekend geschikt, omdat het de mogelijkheid biedt om de belangrijkste factoren en hun invloed te leren kennen [@Shmueli.2010; @Shmueli.2011].

## Factoren

-   `r sConclusie_text`
:::

<!-- Next steps -->

::: {.content-hidden unless-meta="includes.nextsteps"}
# Vervolgstappen: Factoranalyse

De volgende stap (stap 2) is een verdiepende analyse van de mate waarin de **factoren** die we gevonden hebben van invloed zijn op `r sUitval_model`. We kijken naar de rangorde, of ze `r sUitval` verhogen of juist verlagen en hoe stabiel de factoren zijn als we in andere volgordes aan het model toevoegen. Om het concreet te maken zullen we het model toepassen op een aantal fictieve studenten, die we opbouwen uit de meeste voorkomende waarden in deze opleiding. Dit is het onderwerp van analyse 2: de Factoranalyse.
:::

<!-- Referenties -->

# References {.unnumbered}

::: {#refs}
:::

<!-- Verantwoording -->

::: {.content-hidden unless-meta="includes.verantwoording"}
```{r, echo=FALSE, results='asis'}
sQuarto_version <- quarto::quarto_version()
```

 

**Verantwoording**

Deze analyse maakt deel uit van het onderzoek naar kansengelijkheid van het lectoraat Learning Technology & Analytics van De Haagse Hogeschool: [No Fairness without Awareness](https://www.dehaagsehogeschool.nl/onderzoek/kenniscentra/no-fairness-without-awareness) \| Het rapport is door het lectoraat ontwikkeld in [Quarto](https://quarto.org/) `r sQuarto_version`. \| Template versie: `r rmarkdown::metadata$ltatemplate`
:::

<!-- Copyright -->

::: {.content-hidden unless-meta="includes.copyright"}
```{r, echo=FALSE, results='asis'}
nCurrent_year   <- as.numeric(format(Sys.Date(), "%Y"))
```

 

**Copyright**

Dr. Theo Bakker, Lectoraat Learning Technology & Analytics, De Haagse Hogeschool © 2023-`r nCurrent_year` Alle rechten voorbehouden.
:::

<!-- Opschonen -->

```{r, echo = FALSE}
#| label: cleanup

## Datasets
rm(
  dfOpleiding_inschrijvingen,
  dfSummary,
  dfUitval_test,
  dfUitval_train,
  dfUitval_validation,
  splits
  )

## Logistische regressie
if(bInclude_Model_LR) {
  rm(
    lr_auc,
    lr_auc_highest,
    lr_best,
    lr_mod,
    lr_plot,
    lr_recipe,
    lr_res,
    lr_workflow
  )
  # if(sBest_model == "lr") {
  #   rm(last_lr_mod,
  #      last_lr_workflow)
  # }
}

## Random Forest
if(bInclude_Model_RF) {
  rm(
    rf_auc,
    rf_auc_highest,
    rf_best,
    rf_mod,
    rf_recipe,
    rf_res,
    rf_workflow,
    last_rf_mod,
    last_rf_workflow
  )
  # if(sBest_model == "rf") {
  #   rm(last_rf_mod,
  #      last_rf_workflow)
  # }
}

## Final Fit
if(bInclude_Final_fit) {
  rm(
    last_fit
  )
}

## Conclusies
if(bInclude_Conclusies) {
  rm(
    top_models,
    dfTop_vif,
    lTop3_vif,
    lTop45_vif
    )
}

## Collect garbage
invisible(gc())
  
```
