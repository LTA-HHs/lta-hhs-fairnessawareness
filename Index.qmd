---
title: "Prognosemodel - `r params$uitval`"
subtitle: "`r params$faculteit` | `r params$opleidingsnaam` (`r params$opleiding`) - `r params$opleidingsvorm`"

## Auteur en datum
author: "`r params$author`, De HHs"
date: last-modified

## Versie
versie: 1.0

## Format en output
output-file: "lta-hhs-tidymodels-uitval.html"

## Parameters        
params:
  uitval: "Uitval na 1 jaar"
  faculteit: "GVS"
  opleidingsnaam: "B Opleiding tot Verpleegkundige"
  opleiding: "HBO-V"
  opleidingsvorm: "voltijd"
  opleidingsvorm_afkorting: "VT"
  author: "Theo Bakker, lector Learning Technology & Analytics"
---

```{r setup, include = FALSE}

## Sluit het _Setup.R bestand in
source("_Setup.R")

## Config
sUitval_model <- params$uitval

## Maak de variabelen voor de huidige opleiding op basis van de opleidingsnaam en opleidingsvorm
current_opleiding <- Get_Current_opleiding(opleiding = params$opleiding, 
                                           opleidingsvorm = params$opleidingsvorm_afkorting)

## Bepaal op basis hiervan afgeleide variabelen
Set_Current_opleiding_vars(current_opleiding, debug = T)

```

# Inleiding

<!-- H01: Inleiding -->

{{< include "01. Includes/qmdfiles/_Inleiding.qmd" >}}

# Voorbereidingen

## Laad de data

We laden een subset in van historische data specifiek voor:

**Opleiding**: `r params$faculteit` \| `r params$opleidingsnaam` (`r params$opleiding`), `r params$opleidingsvorm`, eerstejaars - **`r sUitval_model`**

```{r}
## Laad de data voor de HBO-V opleiding
dfOpleiding_inschrijvingen_base <- get_lta_studyprogram_enrollments_pin(
    board = "HHs/Inschrijvingen",
    faculty = faculteit,
    studyprogram = opleidingsnaam_huidig,
    studytrack = opleiding,
    studyform = toupper(opleidingsvorm),
    range = "eerstejaars")

## Herschik de levels
Set_Levels(dfOpleiding_inschrijvingen_base)

dfOpleiding_inschrijvingen_base <- dfOpleiding_inschrijvingen_base |>  
  
  ## Maak een eenvoudige Uitval variabele aan
  Mutate_Uitval(sUitval_model) |>
  
  ## Maak van de uitval variabele een factor
  mutate(SUC_Uitval = as.factor(SUC_Uitval)) |> 
  
  ## Herorden de levels van de variabele studiekeuzeprofiel
  mutate(VOP_Studiekeuzeprofiel_LTA_afkorting = fct_relevel(
    VOP_Studiekeuzeprofiel_LTA_afkorting,
    lLevels_skp
  )) |> 
  
  ## Herorden de levels van de variabele aansluiting
  mutate(INS_Aansluiting_LTA = fct_relevel(
    INS_Aansluiting_LTA,
    lLevels_aansluiting
  )) |>
  
  ## Herorden de levels van de variabele vooropleiding
  mutate(VOP_Toelaatgevende_vooropleiding_soort = fct_relevel(
    VOP_Toelaatgevende_vooropleiding_soort,
    lLevels_vop
  ))
  
```

## Selecteer en inspecteer de data

We selecteren eerst de relevante variabelen. We verwijderen daarbij variabelen die maar 1 waarde hebben. We bekijken de variabelen in een samenvatting in relatie tot de Uitval. Daarnaast bekijken we de kwaliteit van de data op missende waarden.

```{r}

## Maak een subset
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen_base |>
  select(
    INS_Student_UUID_opleiding_vorm,
    DEM_Geslacht,
    DEM_Leeftijd_1_oktober,
    GIS_Tijd_fiets_OV,
    INS_Aantal_inschrijvingen,
    INS_Collegejaar,
    INS_Dagen_tussen_aanmelding_en_1_september,
    INS_Aansluiting_LTA,
    INS_Navitas_tf,
    SES_Totaalscore,
    SUC_Uitval,
    VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO,
    VOP_Studiekeuzeprofiel_LTA_afkorting,
    VOP_Toelaatgevende_vooropleiding_soort
  ) |>
  
  ## Hernoem variabelen voor beter leesbare namen
  rename(
    ID                 = INS_Student_UUID_opleiding_vorm,
    Geslacht           = DEM_Geslacht,
    Leeftijd           = DEM_Leeftijd_1_oktober,
    Reistijd           = GIS_Tijd_fiets_OV,
    Dubbele_studie     = INS_Aantal_inschrijvingen,
    Collegejaar        = INS_Collegejaar,
    Aanmelding         = INS_Dagen_tussen_aanmelding_en_1_september,
    Aansluiting        = INS_Aansluiting_LTA,
    Navitas            = INS_Navitas_tf,
    SES_Totaal         = SES_Totaalscore,
    Uitval             = SUC_Uitval,
    Eindcijfer_VO      = VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO,
    Studiekeuzeprofiel = VOP_Studiekeuzeprofiel_LTA_afkorting,
    Vooropleiding      = VOP_Toelaatgevende_vooropleiding_soort
  ) |> 
  
  ## Verwijder variabelen, waarbij er maar 1 waarde is
  select(where(~ n_distinct(.) > 1)) |>
  
  arrange(Collegejaar, ID)

## Verwijder de basis dataset
rm(dfOpleiding_inschrijvingen_base)

## Maak een samenvatting van de data
dfSummary <- dfOpleiding_inschrijvingen |>
  
  ## Verwijder kolommen die niet relevant zijn voor de analyse
  select(-c(ID, Collegejaar)) |> 
  
  ## Pas de labels van Uitval aan van True naar Ja, en van False naar Nee
  mutate(Uitval = fct_recode(Uitval, "Nee" = "FALSE", "Ja" = "TRUE", )) |>
  
  ## Pas de volgorde van de labels van Uitval aan
  mutate(Uitval = fct_relevel(Uitval, "Ja", "Nee")) |>
  
  ## Bouw de samenvatting op op basis van Uitval
  tbl_summary(
    by = Uitval,
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing = "no"
  ) |> 
  
  ## Richt de vormgeving van de table in
  modify_header(all_stat_cols() ~ "**{level}**, N={n} ({style_percent(p)}%)") |> 
  add_p(pvalue_fun = ~ style_pvalue(.x, digits = 2),
        test.args = all_tests("fisher.test") ~ list(simulate.p.value = TRUE)) |> 
  add_overall(last = TRUE, col_label = "**Totaal**, N = {N}") |> 
  add_n()  |> 
  modify_header(label = "**Variabele**") |> 
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Uitval**") |> 
  bold_labels() 

dfSummary

```

```{r}

## Toon een samenvatting van de data, gesorteerd op missende waarden
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(missing_percent, 2)) |>
  arrange(desc(missing_percent)) |>
  knitr::kable()

```

## Bewerk de data

-   Uit de eerste diagnose blijkt dat niet alle variabelen goed genoeg zijn voor het bouwen van een prognosemodel: er zijn missende waarden en niet alle veldtypes zijn geschikt. We passen de variabelen aan zodat we in het model er goed mee kunnen werken.
-   Prognosemodellen kunnen niet omgaan met missende waarden. Om bias te voorkomen verwijderen we geen rijen met missende waarden, maar vullen die op (*imputatie*). We bewerken de data zo dat alle missende waarden worden opgevuld: bij numerieke waarden met het gemiddelde en bij categorische variabelen met 'Onbekend'.
-   We passen sommige variabelen aan, zodat ze in het model gebruikt kunnen worden: tekstvelden zetten we om naar factor; logische variabelen (Ja/Nee) zetten we om naar een numerieke variabele (1/0).
-   De uitkomstvariabele, `Uitval`, leiden we af van de variabele `SUC_Uitval_aantal_jaar_LTA`. Als de waarde daar 1 is, is de student na 1 jaar uitgevallen, 2 na 2 jaar, etc.
-   Een fictief studentnummer (`INS_Student_UUID_opleiding_vorm`) gebruiken we ook, zodat we - als er afwijkende resultaten zijn - de dataset gericht kunnen onderzoeken indien nodig.

```{r}
## Bewerk de data
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
  
  ## Imputeer alle numerieke variabelen met de mean
  mutate(across(where(is.numeric), ~ ifelse(
    is.na(.x),
    mean(.x, na.rm = T),
    .x
  )) ) |>
  
  ## Zet character variabelen om naar factor
  mutate(across(where(is.character), as.factor)) |> 
  
  ## Zet logische variabelen om naar 0 of 1
  mutate(across(where(is.logical), as.integer)) |>
  
  ## Vul in factoren missende waarden op met "Onbekend"
  mutate(across(where(is.factor), ~ suppressWarnings(
    fct_explicit_na(.x, na_level = "Onbekend")
  ))) |> 
  
  ## Herschik de kolommen, zodat Uitval vooraan staat
  select(Uitval, everything()) 
  
## Bekijk de data
glimpse(dfOpleiding_inschrijvingen) 

## Maak een diagnose van de data
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(missing_percent, 2)) |>
  knitr::kable()

```

## Bouw de trainingset, testset en validatieset

-   De data is nu geschikt om een prognosemodel mee te bouwen.
-   Om het model te bouwen, testen en valideren, splitsen we de data in drie delen van 60%, 20% en 20%. We doen dit op zo'n manier, dat elk deel ongeveer een gelijk aantal studenten bevat dat uitvalt.
-   We trainen het model op basis van 60% en testen de modellen tijden het trainen op de overige 20% (de testset).
-   Als het model klaar is, valideren we het op de 20% studenten uit de validatieset, die is opgebouwd uit zo recent mogelijke data. De validatieset blijft dus de gehele tijd ongemoeid, zodat we overfitting - een te goed model op bekende data, maar slechte performance op onbekende data - voorkomen.
-   Een willekeurig, maar vaststaand seedgetal voorkomt dat we bij elke run van het model c.q. deze code een net iets andere uitkomst krijgen.

```{r}
set.seed(0821)

## Splits de data in 3 delen: 60%, 20% en 20%
splits      <- initial_validation_split(dfOpleiding_inschrijvingen,
                                        strata = Uitval,
                                        prop = c(0.6, 0.2))

## Maak drie sets: een trainingset, een testset en een validatieset
dfUitval_train      <- training(splits)
dfUitval_test       <- testing(splits)
dfUitval_validation <- validation_set(splits)

## Training set proporties uitval
dfUitval_train |> 
  count(Uitval) |> 
  mutate(prop = n/sum(n)) |> 
  knitr::kable()

## Test set proporties uitval
dfUitval_test  |> 
  count(Uitval) |> 
  mutate(prop = n/sum(n)) |> 
  knitr::kable()

```

# Model I: Penalized Logistic Regression

-   Het eerste model is een [logistische regressie met penalized likelihood](https://wikistatistiek.amc.nl/Logistische_regressie); we gebruiken de `glmnet` engine voor het bouwen van het model. Penalized likelihood is een techniek die helpt bij het voorkomen van overfitting. [Glmnet](https://glmnet.stanford.edu/articles/glmnet.html) is een populair package voor het bouwen van logistische regressiemodellen.
-   We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric.

## Maak het model

We bouwen eerst het model.

```{r}
#| code-fold: false

## Bouw het model: logistische regressie
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
```

## Maak de recipe

Vervolgens zetten we meerdere stappen in een 'recipe'. We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model. We converteren factoren naar dummy variabelen, verwijderen zero values en centreren en schalen numerieke variabelen.

```{r}
#| code-fold: false

## Bouw de recipe: logistische regressie
lr_recipe <- 
  recipe(Uitval ~ ., data = dfUitval_train) |>  
  update_role(ID, new_role = "ID") |>           ## Zet de student ID als ID variabele
  step_rm(ID, Collegejaar) |>                   ## Verwijder ID en collegejaar uit het model
  step_dummy(all_nominal_predictors()) |>       ## Converteer factoren naar dummy variabelen
  step_zv(all_predictors()) |>                  ## Verwijder zero values
  step_normalize(all_numeric_predictors())      ## Centreer en schaal numerieke variabelen

## Toon de recipe
tidy(lr_recipe) |> 
  knitr::kable()
```

## Maak de workflow

Voor de uitvoering bouwen we een nieuwe workflow. Daaraan voegen we het model en de bewerkingen in de recipe toe.

```{r}
#| code-fold: false

## Maak de workflow: logistische regressie
lr_workflow <- 
  workflow() |>         ## Maak een workflow
  add_model(lr_mod) |>  ## Voeg het model toe
  add_recipe(lr_recipe) ## Voeg de recipe toe

## Toon de workflow
lr_workflow
```

## Tune en train het model

Het model moet getuned worden. Dit houdt in dat we de beste parameters voor het model moeten vinden. We maken een grid met verschillende penalty waarden. Daarmee kunnen we vervolgens het beste model selecteren met de hoogste ROC/AUC. We plotten de resultaten van de tuning, zodat we hieruit het beste model kunnen kiezen.

```{r}
#| code-fold: false

## Maak een grid: logistische regressie
lr_reg_grid <- tibble(penalty = 10 ^ seq(-4, -1, length.out = 30))

## Train en tune het model: logistische regressie
lr_res <- 
  lr_workflow |> 
  tune_grid(dfUitval_validation,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}

## Plot de resultaten
lr_plot <- 
  lr_res |> 
  collect_metrics() |> 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 
```

## Kies het beste model

We evalueren modellen met een zo hoog mogelijke Area under the ROC Curve (AUC/ROC) en een zo laag mogelijke penalty. Zo kunnen we uit de resultaten het beste model kiezen. Tot slot maken we een ROC curve om de prestaties van het model te visualiseren.

```{r}
#| code-fold: false

## Toon het beste model
top_models <-
  lr_res |> 
  show_best(metric = "roc_auc", n = 10) |> 
  mutate(mean = round(mean, 4)) |>
  arrange(penalty) 

top_models|> 
  knitr::kable()

```

```{r}
#| code-fold: false

## Selecteer het beste model: logistische regressie
lr_best <- 
  lr_res |> 
  collect_metrics() |> 
  arrange(penalty) |> 
  slice(1) 

lr_best|> 
  mutate(mean = round(mean, 4)) |>
  knitr::kable()

```

```{r}
#| code-fold: false

## Verzamel de predicties en evalueer het model (AUC/ROC): logistische regressie
lr_auc <- 
  lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

## Bepaal de AUC van het beste model
lr_auc_highest   <-
  lr_res |>
  collect_predictions(parameters = lr_best) |> 
  roc_auc(Uitval, .pred_FALSE)

```

# Model II: Tree-based ensemble

-   Het tweede model is een [random forest](https://en.wikipedia.org/wiki/Random_forest): een ensemble van beslisbomen (decision trees). Het is een krachtig model dat goed om kan gaan met complexe data en veel variabelen.
-   We gebruiken de `ranger` engine voor het bouwen van het model.

## Bepaal het aantal PC-cores

Omdat een random forest model veel berekeningen vereist, willen we daarvoor alle computerkracht gebruiken die beschikbaar is. Het aantal CPU's (*cores*) van de computer bepaalt hoe snel het model getraind kan worden. Deze informatie gebruiken we bij het bouwen van het model.

```{r}

## Bepaal het aantal cores
cores <- parallel::detectCores()

```

## Maak het model

We bouwen eerst het model. We gebruiken de `rand_forest` functie om het model te bouwen. We tunen de `mtry` en `min_n` parameters. De `mtry` parameter bepaalt het aantal variabelen dat per boom wordt gebruikt. De `min_n` parameter bepaalt het minimum aantal observaties dat in een blad van de boom moet zitten. De functie `tune()` is hier nog een *placeholder* om de beste waarden voor deze parameters - die we later bepalen - daar in te stellen. We gebruiken 1.000 bomen c.q. versies van het model.

```{r}
#| code-fold: false

## Bouw het model: random forest

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = cores) |> 
  set_mode("classification")
```

## Maak de recipe

We maken een recipe voor het random forest model. We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model. Overige stappen zijn bij een random forest minder relevant in tegenstelling tot een regressiemodel.

```{r}
#| code-fold: false

## Maak de recipe: random forest
rf_recipe <- 
  recipe(Uitval ~ ., data = dfUitval_train) |> 
  step_rm(ID, Collegejaar)                      ## Verwijder ID en Collegejaar uit het model
  
## Toon de recipe
tidy(rf_recipe) |> 
  knitr::kable()
```

## Maak de workflow

We voegen het model en de recipe toe aan de workflow voor dit model.

```{r}
#| code-fold: false

## Maak de workflow: random forest
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe)

## Toon de workflow
rf_workflow
```

## Tune en train het model

We trainen en tunen het model in de workflow. We maken een grid met verschillende waarden voor de parameters `mtry` en `min_n`. We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric. Met de resultaten van de tuning kiezen we het beste model.

```{r}
#| code-fold: false

## Toon de parameters die getuned kunnen worden
rf_mod

## Extraheer de parameters die getuned worden
extract_parameter_set_dials(rf_mod)

## Bepaal de seed
set.seed(2904)

## Bouw het grid: random forest
rf_res <- 
  rf_workflow |> 
  tune_grid(dfUitval_validation,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

## Kies het beste model

We evalueren de beste modellen en maken een ROC curve om de performance van het model te visualiseren. Vervolgens vergelijken we de prestaties van de modellen en kiezen we het beste model.

```{r}
#| code-fold: false

## Toon de beste modellen
rf_res |> 
  show_best(metric = "roc_auc", n = 15) |> 
  mutate(mean = round(mean, 4)) |>
  knitr::kable()

## Plot de resultaten
autoplot(rf_res)

```

```{r}
#| code-fold: false

## Selecteer het beste model
rf_best <- 
  rf_res |> 
  select_best(metric = "roc_auc")

rf_best|> 
  knitr::kable()

```

```{r}
#| code-fold: true

## Verzamel de predicties
rf_res |> 
  collect_predictions() |> 
  head(10) |>
  knitr::kable()

## Bepaal de AUC/ROC curve
rf_auc <- 
  rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  mutate(model = "Random Forest")

## Bepaal de AUC van het beste model
rf_auc_highest   <-
  rf_res |>
  collect_predictions(parameters = rf_best) |> 
  roc_auc(Uitval, .pred_FALSE)

print(rf_auc_highest)

```

# De uiteindelijke fit

-   In de laatste stap van deze analyse maken we het model definitief.
-   We testen het model op de testset en evalueren het model met metrieken en de Variable Importance Factor (VIF).

## Combineer de AUC/ROC curves en kies het beste model

Eerst combineren we de AUC/ROC curves van de modellen om ze te vergelijken. We kiezen het beste model op basis van de hoogste AUC/ROC: het random forest model.

```{r}

## Combineer de AUC/ROC curves om de modellen te vergelijken
bind_rows(lr_auc, rf_auc) |> 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)

```

```{r}

## Bepaal welke van de modellen het beste is op basis van de hoogste AUC/ROC
lr_auc_highest_value <- lr_auc_highest$.estimate
rf_auc_highest_value <- rf_auc_highest$.estimate

if (lr_auc_highest_value > rf_auc_highest_value) {
  sBest_model <- "Logistische regressie"
  best_model_auc_value <- lr_auc_highest_value
} else {
  sBest_model <- "Random Forest"
  best_model_auc_value <- rf_auc_highest_value
}
```

Het Lineaire regressiemodel heeft een AUC van `r round(lr_auc_highest_value, 4)`; het Random Forest model heeft een AUC van `r round(rf_auc_highest_value, 4)`. Het beste model is het **`r sBest_model`** model met een AUC van `r round(best_model_auc_value, 4)`. We ronden de analyse verder af met het definitieve model.

## Maak het finale model

We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren.

```{r}
#| code-fold: false

## Test het ontwikkelde model op de testset
## Bepaal de optimale parameters voor de random forest op basis van rf_best

## Bouw het laatste model
last_rf_mod <-
  rand_forest(mtry = rf_best$mtry,
              min_n = rf_best$min_n,
              trees = 1000) |>
  set_engine("ranger", num.threads = cores, importance = "impurity") |>
  set_mode("classification")

```

## Maak de workflow

We voegen het model toe aan de workflow en updaten de workflow met het finale model.

```{r}
#| code-fold: false

## Bouw de laatste workflow op basis van het laatste model
last_rf_workflow <- 
  rf_workflow |> 
  update_model(last_rf_mod)

```

## Fit het finale model

We voeren de finale fit uit. De functie `last_fit` past het model toe op de validatieset.

```{r}
#| code-fold: false

## Voer de laatste fit uit
set.seed(2904)

last_rf_fit <- 
  last_rf_workflow |> 
  last_fit(splits)

## Toon de fit
last_rf_fit
```

## Evalueer het finale model: metrieken en vif

We evalueren het finale model op basis van 3 metrieken (accuraatheid, ROC/AUC en de [Brier score](https://en.wikipedia.org/wiki/Brier_score) = de Mean Squared Error) en de Variable Importance Factor (VIF). Uit de VIF is op te maken welke variabelen het meest bijdragen aan de voorspelling van de uitkomstvariabele.

```{r}
#| code-fold: false

## Verzamel de metrieken
last_rf_fit |> 
  collect_metrics() |> 
  mutate(.estimate = round(.estimate, 4)) |>
  knitr::kable()

## Extraheer de feature importance
last_rf_fit |> 
  extract_fit_parsnip() |> 
  vip(num_features = 20)

```

## Plot de ROC curve

Tot slot maken we een ROC curve om de prestaties van het definitieve model te visualiseren. De Sensitivity (True Positive Rate) en Specificity (True Negative Rate) worden hierin uitgezet. De Area under the ROC Curve (AUC/ROC) geeft de prestaties van het model weer. Het model scoort beter naarmate de AUC/ROC dichter bij de 1 ligt (in de linkerbovenhoek). Een AUC/ROC van 0,5 betekent dat het model niet beter presteert dan een willekeurige voorspelling.

```{r}
#| code-fold: false

## Toon de roc curve
last_rf_fit |> 
  collect_predictions() |> 
  roc_curve(Uitval, .pred_FALSE) |> 
  autoplot()

```

# Conclusies

```{r, echo = FALSE}
#| code-fold: false

## Bepaal de accuraatheid van het model, het gemiddelde uitvalpercentage en het base-model
Last_fit_Accuracy   <- last_rf_fit |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pull(.estimate) |>
  round(4) * 100
Avg_Uitval          <- round(mean(dfOpleiding_inschrijvingen$Uitval == "TRUE") * 100, 2) 
Base_Model_Accuracy <- round(100 - Avg_Uitval, 2)
nAccuracy_verschil  <- 1 - abs(Last_fit_Accuracy/Base_Model_Accuracy)

if(Avg_Uitval < 50) {
  Base_Model_Accuracy <- round(100 - Avg_Uitval, 2)
  sUitval <- "viel uit"
} else {
  Base_Model_Accuracy <- Avg_Uitval
  sUitval <- "viel niet uit"
}

## Functies
Get_Accuracy_vergelijking <-
  function(Last_fit_Accuracy, Base_Model_Accuracy) {
    if (Last_fit_Accuracy == Base_Model_Accuracy) {
      "even goed als"
    } else if (Last_fit_Accuracy > Base_Model_Accuracy) {
      "beter"
    } else {
      "slechter"
    }
  }
  
Get_Accuracy_mate <- function(Last_fit_Accuracy) {
  if (Last_fit_Accuracy > 95) {
    "zeer hoog"
  } else if (Last_fit_Accuracy > 90) {
    "hoog"
  } else if (Last_fit_Accuracy > 80) {
    "vrij hoog"
  } else if (Last_fit_Accuracy > 70) {
    "gemiddeld"
  } else if (Last_fit_Accuracy > 60) {
    "vrij laag"
  } else {
    "laag"
  }
}
  
Get_Accuracy_verschil <- function(nAccuracy_verschil) {
  if (nAccuracy_verschil < 0.05) {
    "iets"
  } else if (nAccuracy_verschil < 0.10) {
    "wat"
  } else if (nAccuracy_verschil < 0.20) {
    "een stuk"
  } else {
    "veel"
  }
}

Special_Paste  <- function(vec) sub(",\\s+([^,]+)$", " en \\1", toString(vec))
Print_Variable <- function(variable) paste0("`", variable, "`")

## Bepaal een aantal teksten
sAccuracy_vergelijking <- Get_Accuracy_vergelijking(Last_fit_Accuracy, Base_Model_Accuracy)
sAccuracy_mate         <- Get_Accuracy_mate(Last_fit_Accuracy)
sAccuracy_verschil     <- Get_Accuracy_verschil(nAccuracy_verschil)

## Bepaal de top 3 en top 5 variabelen op basis van de VIF
dfTop_vif   <- last_rf_fit |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  arrange(desc(Importance)) 

lTop3_vif <- dfTop_vif |>
  slice(1:3) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()  

lTop45_vif <- dfTop_vif |>
  ## Regel 4 en 5
  slice(4:5) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()

```

-   **Het beste prognosemodel blijkt een random forest model te zijn.** Van de twee prognosemodellen die we hebben ontwikkeld om `r tolower(sUitval_model)` te voorspellen, lineaire regressie en random forrest, had het random forrest model de hoogste mate van accuraatheid.
-   **De mate van accuraatheid van het model is `r sAccuracy_mate` (`r Last_fit_Accuracy`%).** Het model scoort in de huidige opbouw `r sAccuracy_verschil` `r sAccuracy_vergelijking` dan de accuraatheid van een 'base-model' (`r Base_Model_Accuracy`%). Een base-model neemt de grootste klasse van de gemiddelde `r tolower(sUitval_model)` van de afgelopen jaren als basis: `r Avg_Uitval`% `r sUitval`. Een prognosemodel moet minimaal beter presteren dan een base-model om waarde toe te voegen. Dit is dus vrijwel altijd meer dan de 50% lijn van de AUC/ROC curve.
-   **Het model toont wel aan dat de herkomst van studenten sterker is gecorreleerd met `r tolower(sUitval_model)` dan eerdere prestaties of vooropleiding.** De Variable Importance Factor (VIF) laat namelijk zien dat de variabelen `r lTop3_vif` de 3 belangrijkste variabelen zijn voor het voorspellen van `r tolower(sUitval_model)`, gevolgd door `r lTop45_vif`.

```{r, echo = FALSE}

## Ruim specifieke objecten weer op
rm(
  
  ## Dataset
  dfOpleiding_inschrijvingen,
  dfSummary,
  dfUitval_test,
  dfUitval_train,
  dfUitval_validation,
  splits,
  
  ## Tekst objecten
  top_models,
  dfTop_vif,
  lTop3_vif,
  lTop45_vif,
  
  ## Modelspecifieke objecten
  
  ## Logistische regressie
  lr_auc,
  lr_auc_highest,
  lr_auc_highest_value,
  lr_auc_highest_value,
  lr_best,
  lr_mod,
  lr_plot,
  lr_recipe,
  lr_res,
  lr_workflow,
  
  ## Random Forest
  rf_auc,
  rf_auc_highest,
  rf_auc_highest_value,
  rf_best,
  rf_mod,
  rf_recipe,
  rf_res,
  rf_workflow,
  
  ## Laatste model
  last_rf_fit,
  last_rf_mod,
  last_rf_workflow
  
)

## Collect garbage
invisible(gc())
  
```
